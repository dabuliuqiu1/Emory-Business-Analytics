lines = sc.textFile("s3://671happyzhua273/pagerank.txt")
links = lines.map(lambda url_line: url_line.split(" ")).groupByKey()
ranks = sc.parallelize([("A", 1.7), ("B", 0.9), ("C", 1.3), ("D", 0.2)])

def computeContribs(urls, rank): 
    num_urls = len(urls)
    for url in urls:
        yield (url, rank / num_urls)

def pagerank(itreration_num,contribution,links,ranks):
    for iteration in range(itreration_num):
        contribs = links.join(ranks).flatMap(lambda url_urls_rank: 
                          computeContribs(url_urls_rank[1][0], url_urls_rank[1][1]))
        ranks = contribs.reduceByKey(lambda x,y:x+y).mapValues(lambda rank: rank * contribution + 1 - contribution)
        df = spark.createDataFrame(data=ranks, 
             schema = ["node_name",str(itreration_num)+" iteration + "+str(contribution)+" damp"])
return(df)

a = pagerank(1,0.15,links,ranks)
b = pagerank(1,0.5,links,ranks)
c = pagerank(1,0.85,links,ranks)

ab = a.join(b, ['node_name'])
abc = ab.join(c,['node_name'])
abc.show()

e = pagerank(1,0.85,links,ranks)
f = pagerank(5,0.85,links,ranks)
g = pagerank(10,0.85,links,ranks)
h = pagerank(20,0.85,links,ranks)

ef = e.join(f, ['node_name'])
efg = ef.join(g,['node_name'])
efgh = efg.join(h,['node_name'])

efgh.show()
